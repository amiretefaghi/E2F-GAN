# -*- coding: utf-8 -*-
"""Modules.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1HkUA2Bz2zQjvD3-LkpBGvcMRijUlTtWy
"""

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras.layers import *
import numpy as np

def gated_conv2d(x,kernel_size,filters,strides = (1,1),padding='valid'):
  x_hat = x
  x = Conv2D(filters,kernel_size=kernel_size,strides=strides,padding=padding)(x)
  # x = Activation('relu')(x)
  x = LeakyReLU(0.3)(x)
  x_hat = Conv2D(filters,kernel_size=kernel_size,strides=strides,padding=padding)(x_hat)
  x_hat = Activation('sigmoid')(x_hat)
  x = Multiply()([x,x_hat])
  return x

def IGRB(x,kernel_size,filters,strides = (1,1),padding='same'):
  x_hat1 = x
  x = gated_conv2d(x,kernel_size=kernel_size,filters=filters,strides=strides,padding=padding)
  x_hat2 = x
  x = Conv2D(filters=filters,kernel_size=kernel_size,strides=strides,padding=padding)(x)
  x_hat2 = Conv2D(filters=filters,kernel_size=kernel_size,strides=strides,padding=padding)(x_hat2)
  x_hat2 = Activation('sigmoid')(x_hat2)
  x = Multiply()([x,x_hat2])
  x = Add()([x,x_hat1])
  # x = Activation('relu')(x)
  x = LeakyReLU(0.3)(x)
  x = gated_conv2d(x,kernel_size=kernel_size,filters=filters,strides=strides,padding=padding)

  return x

def CSAB (x):

  alfa = tf.Variable(initial_value=0,trainable=True,dtype=tf.float32)
  x_hat = GlobalAveragePooling2D()(x)
  x_hat = Dense(int(x_hat.shape[1]/4),activation='relu')(x_hat)
  x_hat = Dense(x_hat.shape[1]*4,activation='sigmoid')(x_hat)
  x_hat = Multiply()([x_hat,x])
  q = Conv2D(x.shape[-1]/8,(1,1))(x_hat)
  q = Reshape((x.shape[1]*x.shape[2],int(x.shape[-1]/8)))(q)
  k = Conv2D(x.shape[-1]/8,(1,1))(x_hat)
  k = Reshape((x.shape[1]*x.shape[2],int(x.shape[-1]/8)))(k)
  k = tf.transpose(k,perm=[0,2,1])
  qk = tf.linalg.matmul(q,k)
  qk = tf.nn.softmax(qk)
  v = Conv2D(x.shape[-1],(1,1))(x_hat)
  # v = gated_conv2d(x_hat, (1,1),x.shape[-1],padding='same')
  v = Reshape((x.shape[1]*x.shape[2],x.shape[-1]))(v)
  x_hat = tf.linalg.matmul(qk,v)
  x_hat = Reshape((x.shape[1],x.shape[2],x.shape[-1]))(x_hat)
  x_hat = alfa * x_hat
  x = Add()([x,x_hat])

  # alfa = tf.Variable(initial_value=0,trainable=True,dtype=tf.float32)
  # x_hat = GlobalAveragePooling2D()(x)
  # x_hat = Dense(int(x_hat.shape[1]/4),activation='relu')(x_hat)
  # x_hat = Dense(x_hat.shape[1]*4,activation='sigmoid')(x_hat)
  # q = Conv2D(x.shape[-1]/8,(1,1))(x)
  # q = Reshape((x.shape[1]*x.shape[2],int(x.shape[-1]/8)))(q)
  # k = Conv2D(x.shape[-1]/8,(1,1))(x)
  # k = Reshape((x.shape[1]*x.shape[2],int(x.shape[-1]/8)))(k)
  # k = tf.transpose(k,perm=[0,2,1])
  # qk = tf.linalg.matmul(q,k)
  # qk = tf.nn.softmax(qk)
  # v = Conv2D(x.shape[-1],(1,1))(x)
  # # v = gated_conv2d(x_hat, (1,1),x.shape[-1],padding='same')
  # v = Reshape((x.shape[1]*x.shape[2],x.shape[-1]))(v)
  # x_hat_1 = tf.linalg.matmul(qk,v)
  # x_hat_1 = Reshape((x.shape[1],x.shape[2],x.shape[-1]))(x_hat_1)
  # x_hat_1 = alfa * x_hat_1
  # x = Add()([x,x_hat_1])
  # x = Multiply()([x_hat,x])

  return x

def SPD (x,kernel_size,filters,strides = (1,1),padding='same'):
  x_hat = Conv2D(filters=int(filters/8),kernel_size=kernel_size,
                 strides=strides,padding=padding,
                 dilation_rate=(1,1))(x)
  x_hat = Activation('relu')(x_hat)
  for i in [2,3,4,5,6,8,10]:
    x_hat1 = Conv2D(filters=int(filters/8),kernel_size=kernel_size,
                  strides=strides,padding=padding,
                  dilation_rate=(i,i))(x)
    x_hat1 = Activation('relu')(x_hat1)

    x_hat = Concatenate()([x_hat,x_hat1])
  
  x_hat = Conv2D(filters=filters,kernel_size=kernel_size,
                  strides=strides,padding=padding,
                  dilation_rate=(1,1))(x_hat)
  x_hat = Activation('relu')(x_hat)

  x = Add()([x_hat,x])

  return x

def Self_attention(x):

  alfa = tf.Variable(initial_value=0,trainable=True,dtype=tf.float32)
  x_hat = x
  q = Conv2D(x.shape[-1]/8,(1,1))(x_hat)
  q = Reshape((x.shape[1]*x.shape[2],int(x.shape[-1]/8)))(q)
  k = Conv2D(x.shape[-1]/8,(1,1))(x_hat)
  k = Reshape((x.shape[1]*x.shape[2],int(x.shape[-1]/8)))(k)
  k = tf.transpose(k,perm=[0,2,1])
  qk = tf.linalg.matmul(q,k)
  qk = tf.nn.softmax(qk)
  v = Conv2D(x.shape[-1],(1,1))(x_hat)
  v = Reshape((x.shape[1]*x.shape[2],x.shape[-1]))(v)
  x_hat = tf.linalg.matmul(qk,v)
  x_hat = Reshape((x.shape[1],x.shape[2],x.shape[-1]))(x_hat)
  x_hat = alfa * x_hat
  x = Add()([x,x_hat])

  return x

def SPD_4 (x,kernel_size,filters,strides = (1,1),padding='same'):
  x_hat = Conv2D(filters=int(filters/4),kernel_size=kernel_size,
                 strides=strides,padding=padding,
                 dilation_rate=(1,1))(x)
  x_hat = Activation('relu')(x_hat)
  for i in [2,3,4,5]:
    x_hat1 = Conv2D(filters=int(filters/4),kernel_size=kernel_size,
                  strides=strides,padding=padding,
                  dilation_rate=(i,i))(x)
    x_hat1 = Activation('relu')(x_hat1)

    x_hat = Concatenate()([x_hat,x_hat1])
  
  x_hat = Conv2D(filters=filters,kernel_size=kernel_size,
                  strides=strides,padding=padding,
                  dilation_rate=(1,1))(x_hat)
  x_hat = Activation('relu')(x_hat)

  x = Add()([x_hat,x])

  return x

